{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "\n",
    "def load_dataset(root_path: str):\n",
    "    \"\"\"\n",
    "    Assumes you’ve already extracted:\n",
    "      ../data/vsp_spatial_planning/train_direct.jsonl\n",
    "      ../data/vsp_spatial_planning/img/…\n",
    "    \"\"\"\n",
    "    root = Path(root_path)\n",
    "    jsonl = root / \"train_direct.jsonl\"\n",
    "    if not jsonl.exists():\n",
    "        raise FileNotFoundError(f\"train_direct.jsonl not found under {root}\")\n",
    "    examples = []\n",
    "    with open(jsonl, \"r\") as f:\n",
    "        for line in f:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "def extract_boxed_answer(text: str):\n",
    "    # capture anything inside \\boxed{…}, e.g. \"DOWN\" or \"L,L,R,U,D\"\n",
    "    m = re.search(r\"\\\\boxed\\{([^}]+)\\}\", text)\n",
    "    return m.group(1) if m else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- usage in notebook ---\n",
    "DATA_DIR = Path(\"../data/vsp_spatial_planning\")\n",
    "data = load_dataset(DATA_DIR)\n",
    "print(f\"Total examples: {len(data)}\\n\")\n",
    "\n",
    "# pick one example\n",
    "ex = data[0]\n",
    "print(ex)\n",
    "\n",
    "# 1) print the prompt\n",
    "print(\"=== INPUT TEXT ===\")\n",
    "print(ex[\"text_input\"].strip(), \"\\n\")\n",
    "\n",
    "# 2) resolve & display the two images\n",
    "def resolve_img_path(root: Path, json_field: str):\n",
    "    s = json_field.replace(\"\\\\\", \"\")        # drop any stray backslashes\n",
    "    idx = s.find(\"img/\")\n",
    "    if idx < 0:\n",
    "        raise ValueError(f\"Expected 'img/' in {json_field}\")\n",
    "    return root / s[idx:]\n",
    "\n",
    "img_in_path  = resolve_img_path(DATA_DIR, ex[\"image_input\"])\n",
    "img_out_path = resolve_img_path(DATA_DIR, ex[\"image_output\"])\n",
    "\n",
    "print(\"=== INPUT MAP ===\")\n",
    "display(Image.open(img_in_path).convert(\"RGB\"))\n",
    "\n",
    "print(\"=== HELPER / REASONING PATH MAP ===\")\n",
    "display(Image.open(img_out_path).convert(\"RGB\"))\n",
    "\n",
    "# 3) print the raw text_output and the extracted moves\n",
    "print(\"=== RAW text_output ===\")\n",
    "print(ex[\"text_output\"], \"\\n\")\n",
    "\n",
    "print(\"=== PARSED BOXED ANSWER ===\")\n",
    "print(extract_boxed_answer(ex[\"text_output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regex we expect: one or more of the four moves,\n",
    "# comma-separated, optional spaces\n",
    "MOVE_RE = re.compile(r'^(?:LEFT|RIGHT|UP|DOWN)(?:,\\s*(?:LEFT|RIGHT|UP|DOWN))*$')\n",
    "\n",
    "bad = []\n",
    "for i, ex in enumerate(data):\n",
    "    ans = extract_boxed_answer(ex[\"text_output\"] or \"\")\n",
    "    if ans is None or not MOVE_RE.match(ans):\n",
    "        bad.append((i, ex[\"text_output\"], ans))\n",
    "\n",
    "print(f\"Checked {len(data)} examples.\")\n",
    "if not bad:\n",
    "    print(\"✅ All boxed answers conform to the MOVE pattern.\")\n",
    "else:\n",
    "    print(f\"⚠️ {len(bad)} examples failed format-check:\")\n",
    "    for idx, raw, parsed in bad[:10]:\n",
    "        print(f\"  • idx={idx:4d}  raw={raw!r}  parsed={parsed!r}\")\n",
    "    if len(bad) > 10:\n",
    "        print(\"  …\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4539d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "model     = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=None\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "img1     = Image.open(img_in_path ).convert(\"RGB\")\n",
    "img2 = Image.open(img_out_path).convert(\"RGB\")\n",
    "\n",
    "print(\"INPUT MAP:\")\n",
    "display(img1)\n",
    "\n",
    "print(\"HELPER / REASONING PATH MAP:\")\n",
    "display(img2)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "         {\"type\":\"text\",\n",
    "          \"text\": ex[\"text_input\"].split(\"Here is the map: \")[0] + \"Here is the map: \"},\n",
    "         {\"type\": \"image\", \"image\": img1},    # first <image>\n",
    "         {\"type\":\"text\", \"text\":\"\\nHere is my reasoning path: \"},\n",
    "         {\"type\": \"image\", \"image\": img2},    # second <image>\n",
    "         {\"type\":\"text\",\"text\":\"Please provide your action plan. The final answer MUST BE put in \\\\boxed{}.\"},\n",
    "      ],\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,            \n",
    "    add_generation_prompt=True \n",
    ")\n",
    "\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[prompt],\n",
    "    images=image_inputs,   \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(device)\n",
    "\n",
    "out_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens = 2000,\n",
    "    do_sample      = False,\n",
    ")\n",
    "\n",
    "raw = processor.decode(out_ids[0], skip_special_tokens=True)\n",
    "print(\"❯❯❯ Model’s raw output:\\n\", raw)\n",
    "\n",
    "moves = extract_boxed_answer(raw)\n",
    "print(\"\\n❯❯❯ Parsed moves:\", moves)\n",
    "\n",
    "def normalize_moves(seq: str):\n",
    "    mapping = {\n",
    "        'UP':'U','U':'U',\n",
    "        'DOWN':'D','D':'D',\n",
    "        'LEFT':'L','L':'L',\n",
    "        'RIGHT':'R','R':'R'\n",
    "    }\n",
    "    tokens = re.split(r'[\\s,;]+', (seq or \"\").strip().upper())\n",
    "    return [mapping[t] for t in tokens if t in mapping]\n",
    "\n",
    "pred_moves = normalize_moves(moves)\n",
    "print(\"\\n❯❯❯ Normalized moves:\", pred_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7883a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def make_shifted_copy(in_path, out_path, shift=1):\n",
    "    data = [json.loads(l) for l in open(in_path)]\n",
    "    # extract the helper-image field values\n",
    "    helpers = [ex[\"image_output\"] for ex in data]\n",
    "    # cyclic shift\n",
    "    shifted = helpers[shift:] + helpers[:shift]\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for ex, wrong_helper in zip(data, shifted):\n",
    "            ex_copy = ex.copy()\n",
    "            ex_copy[\"image_output\"] = wrong_helper\n",
    "            f.write(json.dumps(ex_copy, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "make_shifted_copy(\n",
    "    in_path  = \"../data/vsp_spatial_planning/val_split.jsonl\",\n",
    "    out_path = \"../data/vsp_spatial_planning/val_split_shifted.jsonl\",\n",
    "    shift    = 100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edaea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
